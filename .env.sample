#!/bin/bash
################################################################################
# Kubernetes HA Cluster Configuration
#
# Copy this file to .env and customize for your environment:
#   cp .env.sample .env
#   nano .env
#
# This file is sourced by all deployment scripts
################################################################################

#===============================================================================
# CLUSTER CONFIGURATION
#===============================================================================

# Number of nodes to create (minimum 3 for HA, must be odd number for quorum)
NODE_COUNT=3

# Kubernetes version to install
K8S_VERSION="1.29"

# Cluster name (used for VM naming: ${CLUSTER_NAME}-node1, etc.)
CLUSTER_NAME="k8s"

#===============================================================================
# NETWORK MODE CONFIGURATION
#===============================================================================

# Single Interface Mode (Production-Ready Architecture)
# true:  Use single physical interface with multiple IPs (recommended for production)
# false: Use dual interface mode with separate networks (development/testing)
SINGLE_INTERFACE_MODE=false

# Physical interface name (leave empty for auto-detection)
# Examples: eth0, ens18, enp3s0f0
# Auto-detection uses interface with default route
PHYSICAL_INTERFACE=""

#===============================================================================
# NETWORK CONFIGURATION - PRIVATE SUBNET
#===============================================================================

# Private cluster network (used for Kubernetes internal communication)
# VMs will get IPs from this subnet for cluster networking
PRIVATE_SUBNET="192.168.100.0/24"
PRIVATE_GATEWAY="192.168.100.1"

# Starting IP for nodes on private network
# Node 1: 192.168.100.11, Node 2: 192.168.100.12, etc.
PRIVATE_IP_START="192.168.100.11"

# Bridge name for private network (will be created by libvirt in dual-interface mode)
BRIDGE_NAME="br0"

#===============================================================================
# NETWORK CONFIGURATION - EXTERNAL SUBNET
#===============================================================================

# External network (used for accessing services from your main network)
# This is your existing LAN where clients will access services
EXTERNAL_SUBNET="192.168.1.0/24"
EXTERNAL_GATEWAY="192.168.1.1"

# VIP-only mode (recommended to save IPs on external network)
# true:  Nodes only connect secondary NICs to external network, no IPs assigned
#        Only the VIP will be active on the external network (saves 3+ IPs)
# false: Traditional mode - each node gets an IP on external network + VIP
VIP_ONLY_MODE=true

# Starting IP for nodes on external network (only used if VIP_ONLY_MODE=false)
# Node 1: 192.168.1.11, Node 2: 192.168.1.12, etc.
EXTERNAL_IP_START="192.168.1.11"

# Libvirt network name for external network (only used in dual-interface mode)
# Run 'virsh net-list' to see available networks
LIBVIRT_EXTERNAL_NETWORK="default"

# Virtual IP (VIP) for high availability access to services
# This IP will float between nodes via MetalLB
# All services will be accessible via this single IP
VIP="192.168.1.100"

# VIP range for MetalLB IP pool (can be single IP or range)
# For single VIP: "192.168.1.100-192.168.1.100"
# For range: "192.168.1.100-192.168.1.110"
METALLB_IP_RANGE="192.168.1.100-192.168.1.100"

#===============================================================================
# SINGLE INTERFACE MODE SETTINGS
#===============================================================================

# Network architecture when SINGLE_INTERFACE_MODE=true
#
# Option 1: DUAL_SUBNET (both private and external IPs on same interface)
#   - Requires both 192.168.100.0/24 AND 192.168.1.0/24 to be routed on your network
#   - Each node gets both private IP and external IP on same physical NIC
#   - Example: Node1 has 192.168.100.11 and 192.168.1.11 on eth0
#
# Option 2: VIP_ONLY (single subnet with VIP)
#   - Use only one subnet (typically EXTERNAL_SUBNET)
#   - Nodes get single IP, VIP managed by MetalLB
#   - Simpler, works on any network
#
NETWORK_ARCHITECTURE="DUAL_SUBNET"

# Default gateway (used in single-interface mode)
# Should be either PRIVATE_GATEWAY or EXTERNAL_GATEWAY depending on your setup
DEFAULT_GATEWAY="${EXTERNAL_GATEWAY}"

#===============================================================================
# KUBERNETES NETWORK CONFIGURATION
#===============================================================================

# Pod network CIDR (internal to Kubernetes, not routable outside cluster)
POD_CIDR="10.244.0.0/16"

# Service network CIDR (internal to Kubernetes, not routable outside cluster)
SERVICE_CIDR="10.96.0.0/12"

# CNI plugin to use (flannel, calico, cilium)
CNI_PLUGIN="flannel"

#===============================================================================
# VM RESOURCE CONFIGURATION
#===============================================================================

# CPU cores per node
NODE_CPU=4

# RAM per node in GB
NODE_RAM=16

# Disk size per node in GB
NODE_DISK=200

# VM storage pool (where VM disks will be created)
# Leave empty to use default libvirt pool
# Run 'virsh pool-list' to see available pools
STORAGE_POOL=""

#===============================================================================
# IMAGE CONFIGURATION
#===============================================================================

# Ubuntu cloud image URL (22.04 LTS recommended)
CLOUD_IMG_URL="https://cloud-images.ubuntu.com/releases/22.04/release/ubuntu-22.04-server-cloudimg-amd64.img"

# Local path where cloud image will be downloaded (relative to script location)
# Will be created in the same directory as the scripts
CLOUD_IMG_PATH="./ubuntu-22.04-server-cloudimg-amd64.img"

#===============================================================================
# SSH CONFIGURATION
#===============================================================================

# SSH key name (will be created in script directory if doesn't exist)
SSH_KEY_NAME="k8s_rsa"

# SSH key path (relative to script location)
SSH_KEY_PATH="./k8s_rsa"

# Default SSH user for VMs
SSH_USER="k8sadmin"

# Default SSH password (used for initial cloud-init, SSH key is primary method)
SSH_PASSWORD="k8sadmin"

#===============================================================================
# SSL/TLS CONFIGURATION
#===============================================================================

# Enable Let's Encrypt certificates (true/false)
ENABLE_LETSENCRYPT=false

# Domain name for Let's Encrypt (required if ENABLE_LETSENCRYPT=true)
# Example: cluster.example.com
# Services will be: grafana.cluster.example.com, prometheus.cluster.example.com
DOMAIN_NAME=""

# Let's Encrypt email (required if ENABLE_LETSENCRYPT=true)
LETSENCRYPT_EMAIL=""

# Let's Encrypt environment (staging/production)
# Use "staging" for testing to avoid rate limits
LETSENCRYPT_ENV="staging"

#===============================================================================
# METALLB CONFIGURATION
#===============================================================================

# MetalLB version to install
METALLB_VERSION="v0.14.9"

# MetalLB mode (layer2 or bgp)
# layer2: Simple ARP-based, VIP on one node at a time (recommended for most setups)
# bgp: Advanced routing, requires BGP router configuration
METALLB_MODE="layer2"

#===============================================================================
# INGRESS CONFIGURATION
#===============================================================================

# NGINX Ingress Controller version
INGRESS_NGINX_VERSION="v1.11.3"

# Default ingress class
INGRESS_CLASS="nginx"

#===============================================================================
# MONITORING CONFIGURATION
#===============================================================================

# Deploy Prometheus (true/false)
DEPLOY_PROMETHEUS=true

# Deploy Grafana (true/false)
DEPLOY_GRAFANA=true

# Grafana admin password
GRAFANA_ADMIN_PASSWORD="admin"

# Prometheus retention period
PROMETHEUS_RETENTION="15d"

# Prometheus storage size
PROMETHEUS_STORAGE="50Gi"

#===============================================================================
# STORAGE CONFIGURATION
#===============================================================================

# Deploy Longhorn distributed storage (true/false)
DEPLOY_LONGHORN=true

# Longhorn version
LONGHORN_VERSION="v1.7.2"

# Default storage class
DEFAULT_STORAGE_CLASS="longhorn"

# Longhorn data directory (where volumes are stored on each node)
# Use a simple, organized path for easy identification
LONGHORN_DATA_DIR="/var/lib/longhorn"

# Longhorn backup target (for disaster recovery)
# Examples:
#   NFS: nfs://192.168.1.10:/export/longhorn-backups
#   S3:  s3://my-bucket@us-east-1/
# Leave empty to disable backups (not recommended for production)
LONGHORN_BACKUP_TARGET=""

# For S3 backup target (optional, only if using S3)
# LONGHORN_AWS_ACCESS_KEY=""
# LONGHORN_AWS_SECRET_KEY=""

#===============================================================================
# MINIO OBJECT STORAGE CONFIGURATION
#===============================================================================

# Deploy MinIO S3-compatible object storage (true/false)
# MinIO serves as a local S3 backup target for Longhorn within the cluster
# - Single-node (1-3 nodes): Standalone mode with 1 replica
# - Multi-node (4+ nodes): Distributed mode with 4 replicas (HA)
#
# CRITICAL: External S3 replication is MANDATORY for production!
# Without external replication, cluster failure = data + backups lost
DEPLOY_MINIO=true

# MinIO credentials (leave empty for auto-generated password)
MINIO_ROOT_USER="admin"
MINIO_ROOT_PASSWORD=""

# MinIO storage size per replica
MINIO_STORAGE_SIZE="50Gi"

# External S3 for MinIO replication (MANDATORY for production!)
# This is your offsite backup - REQUIRED for disaster recovery
# Supported providers: AWS S3, Backblaze B2, Wasabi, separate MinIO
#
# IMPORTANT: Configure these for production deployments!
# Run ./scripts/configure-minio-replication.sh after deploying MinIO
#
# Examples:
#   AWS S3:       s3.amazonaws.com (or s3.us-west-2.amazonaws.com for specific region)
#   Backblaze B2: s3.us-west-002.backblazeb2.com
#   Wasabi:       s3.wasabisys.com (or s3.us-west-1.wasabisys.com)
MINIO_EXTERNAL_S3_ENDPOINT=""
MINIO_EXTERNAL_S3_ACCESS_KEY=""
MINIO_EXTERNAL_S3_SECRET_KEY=""
MINIO_EXTERNAL_S3_BUCKET="longhorn-backups-offsite"
MINIO_EXTERNAL_S3_REGION="us-east-1"

#===============================================================================
# BACKUP CONFIGURATION
#===============================================================================

# Deploy Velero for backups (true/false)
DEPLOY_VELERO=false

# Velero backup location (requires configuration based on provider)
VELERO_BACKUP_LOCATION=""

#===============================================================================
# LOGGING CONFIGURATION
#===============================================================================

# Log directory (relative to script location)
# Timestamped logs will be created here for each run
LOG_DIR="./logs"

# Log retention in days (logs older than this will be deleted)
LOG_RETENTION_DAYS=30

#===============================================================================
# STATE DIRECTORY
#===============================================================================

# State directory for idempotent operations (relative to script location)
# Scripts will create markers here to track completed steps
STATE_DIR="./.state"

#===============================================================================
# ADVANCED CONFIGURATION
#===============================================================================

# Enable control plane on all nodes (true/false)
# true: All nodes run both control plane and workloads (recommended for HA with minimal nodes)
# false: Dedicated master nodes (requires more nodes)
CONTROL_PLANE_ON_ALL_NODES=true

# API server load balancer endpoint
# For 3-node setup with all control-plane, use first node IP
# This will be replaced with VIP or external LB in production
API_SERVER_ENDPOINT="${PRIVATE_IP_START}:6443"

# Container runtime (containerd or cri-o)
CONTAINER_RUNTIME="containerd"

# Enable IPv6 (true/false)
ENABLE_IPV6=false

# Kubeconfig path (will be created during deployment)
KUBECONFIG_PATH="./kubeconfig"

#===============================================================================
# TIMEOUTS AND RETRIES
#===============================================================================

# SSH connection timeout in seconds
SSH_TIMEOUT=300

# Maximum retries for SSH connection
SSH_MAX_RETRIES=30

# VM boot wait time in seconds
VM_BOOT_WAIT=60

# Kubernetes component readiness timeout in seconds
K8S_READY_TIMEOUT=600

#===============================================================================
# DEBUG OPTIONS
#===============================================================================

# Enable verbose output (true/false)
VERBOSE=false

# Enable debug mode - prints all commands (true/false)
DEBUG=false

# Dry run mode - show what would be done without executing (true/false)
DRY_RUN=false